{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# This file is to finish the udacity nano degree problem for Anna_KaRNNa with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This file is to build a chracter-wise RNN trained on Anna Kerenia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt','r') as f:\n",
    "    text = f.read()\n",
    "vocab =sorted(set(text))\n",
    "vocab_to_int = {c: i for i,c in enumerate(vocab)}\n",
    "int_to_vocab =dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "One of the best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Make training mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Follow the udacity's books function to create the batch-iterator here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns mini-batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def get_batches(arr, batch_size, n_steps):\n",
    "#     ''' Create generator that returns the batches of size \n",
    "#         batch_size * n_steps\n",
    "#         Here we follow the udacity's notebook's rule, we will remove the last piece\n",
    "#         of the samples smaller than the batch_size.\n",
    "#     '''\n",
    "#     size = batch_size * n_steps\n",
    "#     n_batches = len(arr)/size\n",
    "    \n",
    "#     # Here we only keep enough numbers \n",
    "#     arr = arr[:size*n_batches]\n",
    "    \n",
    "#     # reshape batches into batch_size row\n",
    "#     arr = arr.reshape((batch_size,-1))\n",
    "    \n",
    "#     for n in range(0,arr.shape[1],n_steps):\n",
    "#         x = arr[:,n:n+n_steps]\n",
    "#         y_temp = arr[:,n+1:n+n_steps+1]\n",
    "        \n",
    "#         # for the last batch, the y will be short with one.\n",
    "#         # we will pad the last value with 0\n",
    "        \n",
    "#         y = np.zeros(x.shape)\n",
    "#         y[:,:y_temp.shape[1]]=y_temp\n",
    "        \n",
    "#         yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_seqs = 128\n",
    "n_steps = 100\n",
    "label_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x \\n', tensor([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         ...,\n",
      "         [ 0.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 1.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         ...,\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  1.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         ...,\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         ...,\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  1.,  0.],\n",
      "         ...,\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         ...,\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]]))\n",
      "('\\n y \\n', tensor([[ 64,  57,  72,  76,  61,  74,   1,  16,   0,   0],\n",
      "        [  1,  58,  71,  60,  81,   1,  71,  62,   1,  64],\n",
      "        [  1,  76,  71,   1,  69,  61,  27,   1,  37,   1],\n",
      "        [ 57,  76,   1,  57,   1,  63,  77,  65,  68,  76],\n",
      "        [ 61,  69,  58,  61,  74,  65,  70,  63,   1,  76],\n",
      "        [ 71,  78,  65,  76,  59,  64,  11,   1,  79,  64],\n",
      "        [ 62,  71,  74,   1,  76,  61,  70,   1,  81,  61],\n",
      "        [ 65,  70,  75,   1,  57,  74,  61,   1,  75,  57],\n",
      "        [  1,  81,  71,  77,   1,  76,  71,   1,  57,  59],\n",
      "        [ 76,  65,  71,  70,   1,  71,  62,   1,  57,   1]]))\n"
     ]
    }
   ],
   "source": [
    "print('x \\n', x[:10, :10])\n",
    "print('\\n y \\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here we build the LSTM neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Need to rewrite this function\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM_Char(nn.Module):\n",
    "    \n",
    "    def __init__(self,hidden_dim,n_steps,n_layers,label_size,batch_size,drop_out=0.5,lr=0.001):\n",
    "        \n",
    "        super(LSTM_Char,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.label_size = label_size\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_out = drop_out\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.lstm = nn.LSTM(input_size= label_size ,hidden_size = hidden_dim,num_layers = n_layers,\\\n",
    "                            dropout=drop_out,batch_first=True)\n",
    "        self.hidden = self.init_hidden(n_steps)\n",
    "        self.out = nn.Linear(hidden_dim,label_size)\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_hidden(self,n_seqs):\n",
    "        ## Two layer of hidden, first is for the input x, second is for the hidden state\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.n_layers,n_seqs,self.hidden_dim).zero_()),\n",
    "                Variable(weight.new(self.n_layers,n_seqs,self.hidden_dim).zero_()))\n",
    "    \n",
    "    def forward(self,x,hc):\n",
    "        # Here we put the hc in the tuple\n",
    "        x,(h,c) = self.lstm(x,hc)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Put all the LSTM outputs together\n",
    "        x = x.view(x.size()[0]*x.size()[1],self.hidden_dim)\n",
    "        \n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x,(h,c)\n",
    "    \n",
    "    def predict(self,char,h=None,cuda=False,top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Return the predicted character and the hidden state.\n",
    "        '''\n",
    "        # Currently we don't consider cuda here.\n",
    "        if h is None:\n",
    "            h=self.init_hidden(1)\n",
    "        x = np.array([[vocab_to_int[char]]])\n",
    "\n",
    "        x = one_hot_encode(x,self.label_size)\n",
    "        \n",
    "        inputs = Variable(torch.from_numpy(x),volatile = True)\n",
    "        \n",
    "        h = tuple(Variable(each.data,volatile = True) for each in h)\n",
    "        \n",
    "        out,h= self.forward(inputs,h)\n",
    "        \n",
    "        p = F.softmax(out).data\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(self.label_size)\n",
    "        else:\n",
    "            p,top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "            \n",
    "        p = p.numpy().squeeze()\n",
    "\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "\n",
    "        return int_to_vocab[char],h\n",
    "    \n",
    "    def init_weight(self):\n",
    "        initrange = 0.1\n",
    "        #set bias tensor to all zeros\n",
    "        self.out.bias.data.fill_(0)\n",
    "        # set weight to randeom uniform\n",
    "        self.out.weight.data.uniform_(-1,1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    \n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((arr.shape[0],arr.shape[1], n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "net = LSTM_Char(hidden_dim = 512,n_steps=100,\\\n",
    "                n_layers = 2,label_size = len(vocab),\\\n",
    "                batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "net.train()\n",
    "opt = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here we can do the split between training and test data\n",
    "val_frac = 0.2\n",
    "val_index = int(len(encoded)*(1-val_frac))\n",
    "data,val_data = encoded[:val_index],encoded[val_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 6. Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3960)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edwin/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:34: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8278)\n",
      "tensor(3.6796)\n",
      "tensor(3.4764)\n",
      "tensor(3.4779)\n",
      "tensor(3.4492)\n",
      "tensor(3.3558)\n",
      "tensor(3.3411)\n",
      "tensor(3.3469)\n",
      "tensor(3.2965)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edwin/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:46: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/Users/edwin/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:48: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/Users/edwin/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:54: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/Users/edwin/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:58: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch: 1/10...', 'Step: 10...', 'Loss: 3.2965...', 'Val Loss: 3.2920')\n",
      "tensor(3.2986)\n",
      "tensor(3.2863)\n",
      "tensor(3.2664)\n",
      "tensor(3.2839)\n",
      "tensor(3.2500)\n",
      "tensor(3.2333)\n",
      "tensor(3.2243)\n",
      "tensor(3.2257)\n",
      "tensor(3.2096)\n",
      "tensor(3.1942)\n",
      "('Epoch: 1/10...', 'Step: 20...', 'Loss: 3.1942...', 'Val Loss: 3.1983')\n",
      "tensor(3.1910)\n",
      "tensor(3.1775)\n",
      "tensor(3.1793)\n",
      "tensor(3.1542)\n",
      "tensor(3.1202)\n",
      "tensor(3.1387)\n",
      "tensor(3.1163)\n",
      "tensor(3.1192)\n",
      "tensor(3.1044)\n",
      "tensor(3.0662)\n",
      "('Epoch: 1/10...', 'Step: 30...', 'Loss: 3.0662...', 'Val Loss: 3.0821')\n",
      "tensor(3.0642)\n",
      "tensor(3.0456)\n",
      "tensor(3.0302)\n",
      "tensor(3.0290)\n",
      "tensor(3.0011)\n",
      "tensor(3.0034)\n",
      "tensor(2.9824)\n",
      "tensor(2.9437)\n",
      "tensor(2.9210)\n",
      "tensor(2.9130)\n",
      "('Epoch: 1/10...', 'Step: 40...', 'Loss: 2.9130...', 'Val Loss: 2.9193')\n",
      "tensor(2.9033)\n",
      "tensor(2.8961)\n",
      "tensor(2.8451)\n",
      "tensor(2.8567)\n",
      "tensor(2.8358)\n",
      "tensor(2.7893)\n",
      "tensor(2.7747)\n",
      "tensor(2.7620)\n",
      "tensor(2.7464)\n",
      "tensor(2.7399)\n",
      "('Epoch: 1/10...', 'Step: 50...', 'Loss: 2.7399...', 'Val Loss: 2.7589')\n",
      "tensor(2.7378)\n",
      "tensor(2.7304)\n",
      "tensor(2.7091)\n",
      "tensor(2.7046)\n",
      "tensor(2.6854)\n",
      "tensor(2.6399)\n",
      "tensor(2.6586)\n",
      "tensor(2.6536)\n",
      "tensor(2.6034)\n",
      "tensor(2.6055)\n",
      "('Epoch: 1/10...', 'Step: 60...', 'Loss: 2.6055...', 'Val Loss: 2.6313')\n",
      "tensor(2.6158)\n",
      "tensor(2.6188)\n",
      "tensor(2.6251)\n",
      "tensor(2.5979)\n",
      "tensor(2.5746)\n",
      "tensor(2.5899)\n",
      "tensor(2.5662)\n",
      "tensor(2.5389)\n",
      "tensor(2.5440)\n",
      "tensor(2.5411)\n",
      "('Epoch: 1/10...', 'Step: 70...', 'Loss: 2.5411...', 'Val Loss: 2.5603')\n",
      "tensor(2.5198)\n",
      "tensor(2.5399)\n",
      "tensor(2.5081)\n",
      "tensor(2.4934)\n",
      "tensor(2.4929)\n",
      "tensor(2.4787)\n",
      "tensor(2.4847)\n",
      "tensor(2.5056)\n",
      "tensor(2.4758)\n",
      "tensor(2.4820)\n",
      "('Epoch: 1/10...', 'Step: 80...', 'Loss: 2.4820...', 'Val Loss: 2.5066')\n",
      "tensor(2.4655)\n",
      "tensor(2.4598)\n",
      "tensor(2.4620)\n",
      "tensor(2.4360)\n",
      "tensor(2.4572)\n",
      "tensor(2.4406)\n",
      "tensor(2.4759)\n",
      "tensor(2.4726)\n",
      "tensor(2.4981)\n",
      "tensor(2.4240)\n",
      "('Epoch: 1/10...', 'Step: 90...', 'Loss: 2.4240...', 'Val Loss: 2.4623')\n",
      "tensor(2.4275)\n",
      "tensor(2.4111)\n",
      "tensor(2.4009)\n",
      "tensor(2.4065)\n",
      "tensor(2.4275)\n",
      "tensor(2.4616)\n",
      "tensor(2.4062)\n",
      "tensor(2.4365)\n",
      "tensor(2.4094)\n",
      "tensor(2.3932)\n",
      "('Epoch: 1/10...', 'Step: 100...', 'Loss: 2.3932...', 'Val Loss: 2.4256')\n",
      "tensor(2.4100)\n",
      "tensor(2.4114)\n",
      "tensor(2.3816)\n",
      "tensor(2.3756)\n",
      "tensor(2.3857)\n",
      "tensor(2.4130)\n",
      "tensor(2.3540)\n",
      "tensor(2.3974)\n",
      "tensor(2.3775)\n",
      "tensor(2.4070)\n",
      "('Epoch: 1/10...', 'Step: 110...', 'Loss: 2.4070...', 'Val Loss: 2.3910')\n",
      "tensor(2.3664)\n",
      "tensor(2.3510)\n",
      "tensor(2.3382)\n",
      "tensor(2.3646)\n",
      "tensor(2.3488)\n",
      "tensor(2.3604)\n",
      "tensor(2.3146)\n",
      "tensor(2.3482)\n",
      "tensor(2.3006)\n",
      "tensor(2.3100)\n",
      "('Epoch: 1/10...', 'Step: 120...', 'Loss: 2.3100...', 'Val Loss: 2.3610')\n",
      "tensor(2.3291)\n",
      "tensor(2.3115)\n",
      "tensor(2.2943)\n",
      "tensor(2.3245)\n",
      "tensor(2.3330)\n",
      "tensor(2.3003)\n",
      "tensor(2.3251)\n",
      "tensor(2.3031)\n",
      "tensor(2.2729)\n",
      "tensor(2.3018)\n",
      "('Epoch: 2/10...', 'Step: 130...', 'Loss: 2.3018...', 'Val Loss: 2.3296')\n",
      "tensor(2.2924)\n",
      "tensor(2.2820)\n",
      "tensor(2.2896)\n",
      "tensor(2.2670)\n",
      "tensor(2.2720)\n",
      "tensor(2.2943)\n",
      "tensor(2.2967)\n",
      "tensor(2.2944)\n",
      "tensor(2.2771)\n",
      "tensor(2.2769)\n",
      "('Epoch: 2/10...', 'Step: 140...', 'Loss: 2.2769...', 'Val Loss: 2.3053')\n",
      "tensor(2.2940)\n",
      "tensor(2.2734)\n",
      "tensor(2.2541)\n",
      "tensor(2.2365)\n",
      "tensor(2.2598)\n",
      "tensor(2.2349)\n",
      "tensor(2.2335)\n",
      "tensor(2.2135)\n",
      "tensor(2.2145)\n",
      "tensor(2.2380)\n",
      "('Epoch: 2/10...', 'Step: 150...', 'Loss: 2.2380...', 'Val Loss: 2.2802')\n",
      "tensor(2.2404)\n",
      "tensor(2.2049)\n",
      "tensor(2.2191)\n",
      "tensor(2.2173)\n",
      "tensor(2.2175)\n",
      "tensor(2.2312)\n",
      "tensor(2.2104)\n",
      "tensor(2.2306)\n",
      "tensor(2.2142)\n",
      "tensor(2.2191)\n",
      "('Epoch: 2/10...', 'Step: 160...', 'Loss: 2.2191...', 'Val Loss: 2.2525')\n",
      "tensor(2.1883)\n",
      "tensor(2.2051)\n",
      "tensor(2.1848)\n",
      "tensor(2.1881)\n",
      "tensor(2.1981)\n",
      "tensor(2.1722)\n",
      "tensor(2.1646)\n",
      "tensor(2.2050)\n",
      "tensor(2.2127)\n",
      "tensor(2.1553)\n",
      "('Epoch: 2/10...', 'Step: 170...', 'Loss: 2.1553...', 'Val Loss: 2.2300')\n",
      "tensor(2.1496)\n",
      "tensor(2.1668)\n",
      "tensor(2.1712)\n",
      "tensor(2.1850)\n",
      "tensor(2.1577)\n",
      "tensor(2.1844)\n",
      "tensor(2.1599)\n",
      "tensor(2.1596)\n",
      "tensor(2.1932)\n",
      "tensor(2.1640)\n",
      "('Epoch: 2/10...', 'Step: 180...', 'Loss: 2.1640...', 'Val Loss: 2.2052')\n",
      "tensor(2.1609)\n",
      "tensor(2.1729)\n",
      "tensor(2.1249)\n",
      "tensor(2.1563)\n",
      "tensor(2.1560)\n",
      "tensor(2.1582)\n",
      "tensor(2.1835)\n",
      "tensor(2.1484)\n",
      "tensor(2.1567)\n",
      "tensor(2.1536)\n",
      "('Epoch: 2/10...', 'Step: 190...', 'Loss: 2.1536...', 'Val Loss: 2.1851')\n",
      "tensor(2.1299)\n",
      "tensor(2.1211)\n",
      "tensor(2.1214)\n",
      "tensor(2.1337)\n",
      "tensor(2.1049)\n",
      "tensor(2.1379)\n",
      "tensor(2.0999)\n",
      "tensor(2.0977)\n",
      "tensor(2.0918)\n",
      "tensor(2.1003)\n",
      "('Epoch: 2/10...', 'Step: 200...', 'Loss: 2.1003...', 'Val Loss: 2.1620')\n",
      "tensor(2.1033)\n",
      "tensor(2.1227)\n",
      "tensor(2.0919)\n",
      "tensor(2.0782)\n",
      "tensor(2.1039)\n",
      "tensor(2.0876)\n",
      "tensor(2.1052)\n",
      "tensor(2.0662)\n",
      "tensor(2.0894)\n",
      "tensor(2.0891)\n",
      "('Epoch: 2/10...', 'Step: 210...', 'Loss: 2.0891...', 'Val Loss: 2.1420')\n",
      "tensor(2.1159)\n",
      "tensor(2.1224)\n",
      "tensor(2.1364)\n",
      "tensor(2.0735)\n",
      "tensor(2.0690)\n",
      "tensor(2.0744)\n",
      "tensor(2.0453)\n",
      "tensor(2.0670)\n",
      "tensor(2.0683)\n",
      "tensor(2.0982)\n",
      "('Epoch: 2/10...', 'Step: 220...', 'Loss: 2.0982...', 'Val Loss: 2.1204')\n",
      "tensor(2.0703)\n",
      "tensor(2.1022)\n",
      "tensor(2.0823)\n",
      "tensor(2.0828)\n",
      "tensor(2.0737)\n",
      "tensor(2.0703)\n",
      "tensor(2.0374)\n",
      "tensor(2.0617)\n",
      "tensor(2.0530)\n",
      "tensor(2.0530)\n",
      "('Epoch: 2/10...', 'Step: 230...', 'Loss: 2.0530...', 'Val Loss: 2.1016')\n",
      "tensor(2.0242)\n",
      "tensor(2.0733)\n",
      "tensor(2.0439)\n",
      "tensor(2.0736)\n",
      "tensor(2.0541)\n",
      "tensor(2.0291)\n",
      "tensor(2.0249)\n",
      "tensor(2.0675)\n",
      "tensor(2.0228)\n",
      "tensor(2.0551)\n",
      "('Epoch: 2/10...', 'Step: 240...', 'Loss: 2.0551...', 'Val Loss: 2.0801')\n",
      "tensor(1.9945)\n",
      "tensor(2.0297)\n",
      "tensor(1.9928)\n",
      "tensor(2.0068)\n",
      "tensor(2.0155)\n",
      "tensor(1.9909)\n",
      "tensor(1.9807)\n",
      "tensor(2.0264)\n",
      "tensor(2.0371)\n",
      "tensor(2.0082)\n",
      "('Epoch: 3/10...', 'Step: 250...', 'Loss: 2.0082...', 'Val Loss: 2.0609')\n",
      "tensor(2.0451)\n",
      "tensor(2.0175)\n",
      "tensor(1.9687)\n",
      "tensor(1.9896)\n",
      "tensor(1.9944)\n",
      "tensor(2.0114)\n",
      "tensor(1.9934)\n",
      "tensor(1.9845)\n",
      "tensor(1.9699)\n",
      "tensor(1.9835)\n",
      "('Epoch: 3/10...', 'Step: 260...', 'Loss: 1.9835...', 'Val Loss: 2.0502')\n",
      "tensor(2.0071)\n",
      "tensor(2.0037)\n",
      "tensor(2.0045)\n",
      "tensor(2.0019)\n",
      "tensor(2.0200)\n",
      "tensor(1.9931)\n",
      "tensor(1.9834)\n",
      "tensor(1.9643)\n",
      "tensor(1.9806)\n",
      "tensor(1.9863)\n",
      "('Epoch: 3/10...', 'Step: 270...', 'Loss: 1.9863...', 'Val Loss: 2.0320')\n",
      "tensor(1.9635)\n",
      "tensor(1.9360)\n",
      "tensor(1.9423)\n",
      "tensor(1.9545)\n",
      "tensor(1.9694)\n",
      "tensor(1.9499)\n",
      "tensor(1.9537)\n",
      "tensor(1.9526)\n",
      "tensor(1.9404)\n",
      "tensor(1.9588)\n",
      "('Epoch: 3/10...', 'Step: 280...', 'Loss: 1.9588...', 'Val Loss: 2.0208')\n",
      "tensor(1.9611)\n",
      "tensor(1.9650)\n",
      "tensor(1.9733)\n",
      "tensor(1.9621)\n",
      "tensor(1.9132)\n",
      "tensor(1.9502)\n",
      "tensor(1.9426)\n",
      "tensor(1.9508)\n",
      "tensor(1.9298)\n",
      "tensor(1.9393)\n",
      "('Epoch: 3/10...', 'Step: 290...', 'Loss: 1.9393...', 'Val Loss: 2.0013')\n",
      "tensor(1.9302)\n",
      "tensor(1.9545)\n",
      "tensor(1.9702)\n",
      "tensor(1.9094)\n",
      "tensor(1.9208)\n",
      "tensor(1.9251)\n",
      "tensor(1.9312)\n",
      "tensor(1.9437)\n",
      "tensor(1.9174)\n",
      "tensor(1.9468)\n",
      "('Epoch: 3/10...', 'Step: 300...', 'Loss: 1.9468...', 'Val Loss: 1.9858')\n",
      "tensor(1.9279)\n",
      "tensor(1.9243)\n",
      "tensor(1.9529)\n",
      "tensor(1.9238)\n",
      "tensor(1.9409)\n",
      "tensor(1.9424)\n",
      "tensor(1.8922)\n",
      "tensor(1.9223)\n",
      "tensor(1.9253)\n",
      "tensor(1.9280)\n",
      "('Epoch: 3/10...', 'Step: 310...', 'Loss: 1.9280...', 'Val Loss: 1.9747')\n",
      "tensor(1.9427)\n",
      "tensor(1.9141)\n",
      "tensor(1.9372)\n",
      "tensor(1.9408)\n",
      "tensor(1.9095)\n",
      "tensor(1.9085)\n",
      "tensor(1.9011)\n",
      "tensor(1.9183)\n",
      "tensor(1.8903)\n",
      "tensor(1.9260)\n",
      "('Epoch: 3/10...', 'Step: 320...', 'Loss: 1.9260...', 'Val Loss: 1.9634')\n",
      "tensor(1.8858)\n",
      "tensor(1.8841)\n",
      "tensor(1.8857)\n",
      "tensor(1.8798)\n",
      "tensor(1.9074)\n",
      "tensor(1.9124)\n",
      "tensor(1.8700)\n",
      "tensor(1.8769)\n",
      "tensor(1.8982)\n",
      "tensor(1.8828)\n",
      "('Epoch: 3/10...', 'Step: 330...', 'Loss: 1.8828...', 'Val Loss: 1.9514')\n",
      "tensor(1.9178)\n",
      "tensor(1.8737)\n",
      "tensor(1.8920)\n",
      "tensor(1.8790)\n",
      "tensor(1.9161)\n",
      "tensor(1.9216)\n",
      "tensor(1.9204)\n",
      "tensor(1.8852)\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "clip = 5\n",
    "counter = 0\n",
    "print_every = 10\n",
    "for e in range(epoch):\n",
    "    h = net.init_hidden(n_seqs)\n",
    "    for x, y in get_batches(data,n_seqs,n_steps):\n",
    "        counter += 1\n",
    "        \n",
    "        # Need to do one-hot encoding and make them the torch tensors\n",
    "        x = one_hot_encode(x,label_size)\n",
    "        x, y =torch.from_numpy(x),torch.from_numpy(y)\n",
    "        \n",
    "        inputs,targets = Variable(x), Variable(y)\n",
    "        \n",
    "        # Create new variables for the hidden state, otherwise we'd backprop\n",
    "        # thourgh the entire training history\n",
    "        ## Need to double check which options works better, detach or create new variables\n",
    "        \n",
    "        h = tuple([Variable(each.data) for each in h])\n",
    "        \n",
    "        net.zero_grad()\n",
    "        \n",
    "        output, h = net.forward(inputs,h)\n",
    "        \n",
    "        loss = criterion(output,targets.view(n_seqs*n_steps))\n",
    "        \n",
    "        print loss\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # clip_grad_norm helps prevent the exploding gradient\n",
    "        # double check this\n",
    "        nn.utils.clip_grad_norm(net.parameters(),clip)\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(n_seqs)\n",
    "            val_losses = []\n",
    "            for x, y in get_batches(val_data,n_seqs,n_steps):\n",
    "                x = one_hot_encode(x,label_size)\n",
    "                x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
    "                \n",
    "                val_h = tuple([Variable(each.data,volatile= True) for each in val_h ])\n",
    "                \n",
    "                inputs, targets = Variable(x,volatile=True),Variable(y,volatile=True)\n",
    "                \n",
    "                output,val_h = net.forward(inputs,val_h)\n",
    "                \n",
    "                val_loss = criterion(output,targets.view(n_seqs*n_steps))\n",
    "                \n",
    "                val_losses.append(val_loss.data[0])\n",
    "                \n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.4f}...\".format(loss.data[0]),\n",
    "                  \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 2000\n",
    "prime='Anna'\n",
    "top_k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chars = [ch for ch in prime]\n",
    "h = net.init_hidden(1)\n",
    "for ch in prime:\n",
    "    char, h = net.predict(ch, h, cuda=False, top_k=top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars.append(char)\n",
    "\n",
    "# Now pass in the previous character and get a new one\n",
    "for ii in range(size):\n",
    "    char, h = net.predict(chars[-1], h, cuda=False, top_k=top_k)\n",
    "    chars.append(char)\n",
    "\n",
    "print ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
